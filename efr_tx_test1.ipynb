{"cells":[{"cell_type":"markdown","metadata":{},"source":["# layers.py"]},{"cell_type":"code","execution_count":6,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-10-31T15:14:32.392941Z","iopub.status.busy":"2023-10-31T15:14:32.392576Z","iopub.status.idle":"2023-10-31T15:14:32.449474Z","shell.execute_reply":"2023-10-31T15:14:32.448549Z","shell.execute_reply.started":"2023-10-31T15:14:32.392913Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import math\n","\n","class myRNN(nn.Module):\n","    def __init__(self,input_size,hidden_size,num_layers,dp=0,bd=False):\n","        super(myRNN,self).__init__()\n","        self.hidden_dim = hidden_size\n","        self.n_layers = num_layers\n","        self.RNN = nn.GRU(input_size = input_size,hidden_size=hidden_size,num_layers=num_layers,dropout=dp,batch_first=True,bidirectional=bd)\n","       \n","    def forward(self,x,h0=None):\n","        out,h = self.RNN(x,h0)\n","        return out,h\n","\n","class attention(nn.Module):\n","    def __init__(self,qembed_dim, kembed_dim=None, vembed_dim=None, hidden_dim=None, out_dim=None, dropout=0):\n","        super(attention, self).__init__()\n","        if kembed_dim is None:\n","            kembed_dim = qembed_dim\n","        if hidden_dim is None:\n","            hidden_dim = kembed_dim\n","        if out_dim is None:\n","            out_dim = kembed_dim\n","        if vembed_dim is None:\n","            vembed_dim = kembed_dim\n","            \n","        self.qembed_dim = qembed_dim\n","        self.kembed_dim = kembed_dim\n","        self.vembed_dim = vembed_dim\n","        \n","        self.hidden_dim = hidden_dim\n","        self.for_key = nn.Linear(kembed_dim,hidden_dim)\n","        self.for_query = nn.Linear(qembed_dim,hidden_dim)\n","        self.for_value = nn.Linear(vembed_dim,hidden_dim)\n","        self.normalise_factor = hidden_dim**(1/2)\n","    \n","    def mask_score(self,s,m):\n","        for i in range(s.size()[0]):\n","            for j in range(s.size()[1]):\n","                for k in range(s.size()[2]):\n","                    if m[i][j][k] == 0:\n","                        s[i][j][k] = float('-inf')   #So that after softmax, 0 weight is given to it\n","        return s\n","    \n","    def forward(self,key,query,mask=None):\n","        if len(query.shape) == 1:\n","            query = torch.unsqueeze(query, dim=0)\n","        if len(key.shape) == 1:\n","            key = torch.unsqueeze(key, dim=0)\n","            \n","        if len(query.shape) == 2:\n","            query = torch.unsqueeze(query, dim=1)\n","        if len(key.shape) == 2:\n","            key = torch.unsqueeze(key, dim=1)\n","            \n","        new_query = self.for_query(query)\n","        new_key = self.for_key(key)\n","        new_value = self.for_value(key)\n","        \n","        score = torch.bmm(new_query,new_key.permute(0,2,1))/self.normalise_factor\n","        \n","        if mask != None:\n","            score = self.mask_score(score,mask)\n","            \n","        score = F.softmax(score,-1)\n","        score.data[score!=score] = 0         #removing nan values\n","        \n","        output = torch.bmm(score,new_value)\n","        return output,score\n","\n","class interact(nn.Module):\n","    def __init__(self,hidden_dim,weight_matrix,utt2idx):\n","        super(interact, self).__init__()\n","        self.hidden_size = hidden_dim\n","\n","        self.embedding, num_embeddings, embedding_dim = create_emb_layer(weight_matrix,utt2idx)\n","        self.rnnD = myRNN(embedding_dim, hidden_dim,1)   #Dialogue\n","        self.drop1 = nn.Dropout()\n","        \n","        self.rnnG = myRNN(embedding_dim*3, hidden_dim,1)   #Global level\n","        self.drop2 = nn.Dropout()\n","        \n","        self.attn = attention(embedding_dim)\n","        \n","        self.rnnS = myRNN(embedding_dim*2, embedding_dim*2,1)   #Speaker representation\n","        self.drop3 = nn.Dropout()\n","\n","    def forward(self, chat_ids, speaker_info, sp_dialogues, sp_ind, inputs):\n","        whole_dialogue_indices = inputs\n","        \n","        bert_embs = self.embedding(whole_dialogue_indices)\n","               \n","        dialogue, h1 = self.rnnD(bert_embs)    #Get global level representation\n","        dialogue = self.drop1(dialogue)\n","\n","        device = inputs.device\n","        \n","        fop = torch.zeros((dialogue.size()[0],dialogue.size()[1],dialogue.size()[2])).to(device)\n","        fop2 = torch.zeros((dialogue.size()[0],dialogue.size()[1],dialogue.size()[2]*3)).to(device)\n","        op = torch.zeros((dialogue.size()[0],dialogue.size()[1],dialogue.size()[2])).to(device)\n","        spop = torch.zeros((dialogue.size()[0],dialogue.size()[1],dialogue.size()[2]*2)).to(device)\n","                    \n","        h0 = torch.randn(1, 1, self.hidden_size*2).to(device)\n","        d_h = torch.randn(1, 1, self.hidden_size).to(device)\n","        attn_h = torch.randn(1, 1, self.hidden_size).to(device)\n","        \n","        for b in range(dialogue.size()[0]):\n","            d_id = chat_ids[b]\n","            speaker_hidden_states = {}\n","            for s in range(dialogue.size()[1]):\n","                fop = op.clone()\n","                \n","                current_utt = dialogue[b][s]\n","                \n","                current_speaker = speaker_info[d_id][s]\n","                \n","                if current_speaker not in speaker_hidden_states:\n","                    speaker_hidden_states[current_speaker] = h0\n","                \n","                h = speaker_hidden_states[current_speaker]\n","                current_utt_emb = torch.unsqueeze(torch.unsqueeze(current_utt,0),0)\n","                \n","                key = fop[b][:s+1].clone()\n","                key = torch.unsqueeze(key,0)\n","                \n","                if s == 0:\n","                    tmp = torch.cat([attn_h,current_utt_emb],-1).to(device)\n","                    spop[b][s], h_new = self.rnnS(tmp,h)\n","                else:\n","                    query = current_utt_emb\n","                    attn_op,_ = self.attn(key,query)\n","                    \n","                    tmp = torch.cat([attn_op,current_utt_emb],-1).to(device)\n","                    spop[b][s], h_new = self.rnnS(tmp,h)\n","                \n","                spop[b][s] = spop[b][s].add(tmp)        # Residual Connection        \n","                speaker_hidden_states[current_speaker] = h_new\n","                \n","                fop2[b][s] = torch.cat([spop[b][s],dialogue[b][s]],-1)\n","                tmp = torch.unsqueeze(torch.unsqueeze(fop2[b][s].clone(),0),0)\n","                op[b][s],d_h = self.rnnG(tmp,d_h)\n","\n","        return op,spop\n","    \n","class fc_e(nn.Module):\n","    def __init__(self,inp_dim,op_dim):\n","        super(fc_e,self).__init__()\n","        self.linear1 = nn.Linear(inp_dim,int(inp_dim/2))\n","        self.drop1 = nn.Dropout()\n","        \n","        self.linear2 = nn.Linear(int(inp_dim/2),int(inp_dim/4))\n","        self.drop2 = nn.Dropout(0.6)\n","        \n","        self.linear3 = nn.Linear(int(inp_dim/4),op_dim)\n","        self.drop3 = nn.Dropout(0.7)\n","    def forward(self,x):\n","        ip = x.float()\n","    \n","        op = self.linear1(ip)\n","        op = self.drop1(op)\n","        \n","        op = self.linear2(op)\n","        op = self.drop2(op)\n","        \n","        op = self.linear3(op)\n","        op = self.drop3(op)\n","        \n","        return op\n","\n","class fc_t(nn.Module):\n","    def __init__(self,inp_dim,op_dim):\n","        super(fc_t,self).__init__()\n","        self.linear1 = nn.Linear(inp_dim,inp_dim)\n","        self.drop1 = nn.Dropout(0.7)\n","        \n","        self.linear2 = nn.Linear(inp_dim,inp_dim)\n","        self.drop2 = nn.Dropout(0.7)\n","        \n","        self.linear3 = nn.Linear(inp_dim,int(inp_dim/2))\n","        self.drop3 = nn.Dropout(0.7)\n","        \n","        self.linear4 = nn.Linear(int(inp_dim/2),int(inp_dim/4))\n","        self.drop4 = nn.Dropout(0.7)\n","        \n","        self.linear5 = nn.Linear(int(inp_dim/4),op_dim)\n","        self.drop5 = nn.Dropout(0.7)\n","    def forward(self,x):\n","        ip = x.float()\n","    \n","        op = self.linear1(ip)\n","        op = self.drop1(op)\n","        \n","        op = self.linear2(ip)\n","        op = self.drop2(op)\n","        \n","        op = self.linear3(ip)\n","        op = self.drop3(op)\n","        \n","        op = self.linear4(op)\n","        op = self.drop4(op)\n","        \n","        op = self.linear5(op)\n","        op = self.drop5(op)\n","        \n","        return op\n","    \n","class maskedattn(nn.Module):\n","    def __init__(self,batch_size, s_len, emb_size):\n","        super(maskedattn,self).__init__()\n","        self.b_len = batch_size\n","        self.s_len = s_len\n","        self.emb_size = emb_size\n","        self.attn = attention(emb_size*2, kembed_dim=emb_size, out_dim=emb_size)\n","    \n","    def create_mask(self,n):\n","        mask = torch.zeros((1, self.s_len, self.emb_size), dtype=torch.uint8)\n","        mask[:n+1] = torch.ones((self.emb_size), dtype=torch.uint8)\n","        mask = mask.repeat(self.b_len,1,1)\n","        return mask\n","        \n","    def forward(self,key,query):\n","        device = key.device\n","\n","        ops = torch.zeros([key.size()[0],key.size()[1], key.size()[2]], dtype=torch.float32).to(device)\n","        for i in range(key.size()[1]):\n","          mask = self.create_mask(i)\n","          op,_ = self.attn(key,query,mask=mask)\n","          for b in range(op.size()[0]):\n","            ops[b][i] = op[b][i]\n","        return ops\n","    \n","class memnet(nn.Module):\n","  def __init__(self,num_hops,hidden_size,batch_size,seq_len):\n","    super(memnet,self).__init__()\n","    self.num_hops = num_hops\n","    self.rnn = myRNN(hidden_size, hidden_size, 1)\n","    self.masked_attention = maskedattn(batch_size,seq_len,hidden_size)\n","  \n","  def forward(self,globl,spl):\n","    X = globl\n","    for hop in range(self.num_hops):\n","      dialogue,h = self.rnn(X)\n","      X = self.masked_attention(dialogue,spl)\n","    return X\n","\n","class pool(nn.Module):\n","    def __init__(self,mode=\"mean\"):\n","        super(pool,self).__init__()\n","        self.mode = mode\n","    def forward(self,x):\n","        device = x.device\n","        op = torch.zeros((x.size()[0],x.size()[1],x.size()[2])).to(device)\n","        for b in range(x.size()[0]):\n","            this_tensor = []\n","            for s in range(x.size()[1]):\n","                this_tensor.append(x[b][s])\n","                if self.mode == \"mean\":\n","                    op[b][s] = torch.mean(torch.stack(this_tensor),0)\n","                elif self.mode == \"max\":\n","                    op[b][s],_ = torch.max(torch.stack(this_tensor),0)\n","                elif self.mode == \"sum\":\n","                    op[b][s] = torch.sum(torch.stack(this_tensor),0)\n","                else:\n","                    print(\"Error: Mode can be either mean or max only\")\n","        return op\n","\n","class PositionalEncoding(nn.Module):\n","    def __init__(self, d_model, dropout=0.1, max_len=5000):\n","        super(PositionalEncoding, self).__init__()\n","        self.dropout = nn.Dropout(p=dropout)\n","        pe = torch.zeros(max_len, d_model)\n","        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n","        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n","        pe[:, 0::2] = torch.sin(position * div_term)\n","        pe[:, 1::2] = torch.cos(position * div_term)\n","        pe = pe.unsqueeze(0).transpose(0, 1)\n","        self.register_buffer('pe', pe)\n","\n","    def forward(self, x):\n","        x = x + self.pe[:x.size(0), :]\n","        return self.dropout(x)"]},{"cell_type":"markdown","metadata":{},"source":["# Pickle Data Loader.py"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-10-31T15:14:32.511841Z","iopub.status.busy":"2023-10-31T15:14:32.511541Z","iopub.status.idle":"2023-10-31T15:14:32.532080Z","shell.execute_reply":"2023-10-31T15:14:32.531130Z","shell.execute_reply.started":"2023-10-31T15:14:32.511818Z"},"trusted":true},"outputs":[],"source":["pickle_folder_path = \"/kaggle/input/meld-pickles/Pickles/\"\n","\n","import pickle\n","\n","def load_erc():\n","    with open(pickle_folder_path + \"idx2utt.pickle\",\"rb\") as f:\n","        idx2utt = pickle.load(f)\n","    with open(pickle_folder_path + \"utt2idx.pickle\",\"rb\") as f:\n","        utt2idx = pickle.load(f)\n","        \n","    with open(pickle_folder_path + \"idx2emo.pickle\",\"rb\") as f:\n","        idx2emo = pickle.load(f)\n","    with open(pickle_folder_path + \"emo2idx.pickle\",\"rb\") as f:\n","        emo2idx = pickle.load(f)\n","        \n","    with open(pickle_folder_path + \"idx2speaker.pickle\",\"rb\") as f:\n","        idx2speaker = pickle.load(f)\n","    with open(pickle_folder_path + \"speaker2idx.pickle\",\"rb\") as f:\n","        speaker2idx = pickle.load(f)\n","\n","    with open(pickle_folder_path + \"weight_matrix.pickle\",\"rb\") as f:\n","        weight_matrix = pickle.load(f)\n","\n","    with open(pickle_folder_path + \"train_data.pickle\",\"rb\") as f:\n","        my_dataset_train = pickle.load(f)\n","        \n","    with open(pickle_folder_path + \"test_data.pickle\",\"rb\") as f:\n","        my_dataset_test = pickle.load(f)\n","        \n","    with open(pickle_folder_path + \"final_speaker_info.pickle\",\"rb\") as f:\n","        final_speaker_info = pickle.load(f)\n","        \n","    with open(pickle_folder_path + \"final_speaker_dialogues.pickle\",\"rb\") as f:\n","        final_speaker_dialogues = pickle.load(f)\n","        \n","    with open(pickle_folder_path + \"final_speaker_emotions.pickle\",\"rb\") as f:\n","        final_speaker_emotions = pickle.load(f)\n","        \n","    with open(pickle_folder_path + \"final_speaker_indices.pickle\",\"rb\") as f:\n","        final_speaker_indices = pickle.load(f)\n","        \n","    with open(pickle_folder_path + \"final_utt_len.pickle\",\"rb\") as f:\n","        final_utt_len = pickle.load(f)\n","\n","    return idx2utt, utt2idx, idx2emo, emo2idx, idx2speaker,\\\n","        speaker2idx, weight_matrix, my_dataset_train, my_dataset_test,\\\n","        final_speaker_info, final_speaker_dialogues, final_speaker_emotions,\\\n","        final_speaker_indices, final_utt_len\n","\n","def load_efr():\n","    with open(pickle_folder_path + \"idx2utt.pickle\",\"rb\") as f:\n","        idx2utt = pickle.load(f)\n","    with open(pickle_folder_path + \"utt2idx.pickle\",\"rb\") as f:\n","        utt2idx = pickle.load(f)\n","        \n","    with open(pickle_folder_path + \"idx2emo.pickle\",\"rb\") as f:\n","        idx2emo = pickle.load(f)\n","    with open(pickle_folder_path + \"emo2idx.pickle\",\"rb\") as f:\n","        emo2idx = pickle.load(f)\n","        \n","    with open(pickle_folder_path + \"idx2speaker.pickle\",\"rb\") as f:\n","        idx2speaker = pickle.load(f)\n","    with open(pickle_folder_path + \"speaker2idx.pickle\",\"rb\") as f:\n","        speaker2idx = pickle.load(f)\n","\n","    with open(pickle_folder_path + \"weight_matrix.pickle\",\"rb\") as f:\n","        weight_matrix = pickle.load(f)\n","\n","    with open(pickle_folder_path + \"train_data_trig.pickle\",\"rb\") as f:\n","        my_dataset_train = pickle.load(f)\n","\n","    with open(pickle_folder_path + \"test_data_trig.pickle\",\"rb\") as f:\n","        my_dataset_test = pickle.load(f)\n","        \n","    with open(pickle_folder_path + \"global_speaker_info_trig.pickle\",\"rb\") as f:\n","        global_speaker_info = pickle.load(f)\n","        \n","    with open(pickle_folder_path + \"speaker_dialogues_trig.pickle\",\"rb\") as f:\n","        speaker_dialogues = pickle.load(f)\n","        \n","    with open(pickle_folder_path + \"speaker_emotions_trig.pickle\",\"rb\") as f:\n","        speaker_emotions = pickle.load(f)\n","        \n","    with open(pickle_folder_path + \"speaker_indices_trig.pickle\",\"rb\") as f:\n","        speaker_indices = pickle.load(f)\n","        \n","    with open(pickle_folder_path + \"utt_len_trig.pickle\",\"rb\") as f:\n","        utt_len = pickle.load(f)\n","        \n","    with open(pickle_folder_path + \"global_speaker_info_test_trig.pickle\",\"rb\") as f:\n","        global_speaker_info_test = pickle.load(f)\n","        \n","    with open(pickle_folder_path + \"speaker_dialogues_test_trig.pickle\",\"rb\") as f:\n","        speaker_dialogues_test = pickle.load(f)\n","        \n","    with open(pickle_folder_path + \"speaker_emotions_test_trig.pickle\",\"rb\") as f:\n","        speaker_emotions_test = pickle.load(f)\n","        \n","    with open(pickle_folder_path + \"speaker_indices_test_trig.pickle\",\"rb\") as f:\n","        speaker_indices_test = pickle.load(f)\n","        \n","    with open(pickle_folder_path + \"utt_len_test_trig.pickle\",\"rb\") as f:\n","        utt_len_test = pickle.load(f)\n","\n","    return idx2utt, utt2idx, idx2emo, emo2idx, idx2speaker,\\\n","        speaker2idx, weight_matrix, my_dataset_train, my_dataset_test,\\\n","        global_speaker_info, speaker_dialogues, speaker_emotions, \\\n","        speaker_indices, utt_len, global_speaker_info_test, speaker_dialogues_test, \\\n","        speaker_emotions_test, speaker_indices_test, utt_len_test"]},{"cell_type":"markdown","metadata":{},"source":["# utils.py"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-10-31T15:14:32.534113Z","iopub.status.busy":"2023-10-31T15:14:32.533851Z","iopub.status.idle":"2023-10-31T15:14:32.548336Z","shell.execute_reply":"2023-10-31T15:14:32.547470Z","shell.execute_reply.started":"2023-10-31T15:14:32.534091Z"},"trusted":true},"outputs":[],"source":["import torch.nn as nn\n","\n","##Source: https://medium.com/@martinpella/how-to-use-pre-trained-word-embeddings-in-pytorch-71ca59249f76\n","def create_emb_layer(weights_matrix, utt2idx, non_trainable=False):\n","    num_embeddings, embedding_dim = weights_matrix.size()\n","    emb_layer = nn.Embedding(num_embeddings, embedding_dim, padding_idx=utt2idx[\"<pad>\"])\n","    emb_layer.load_state_dict({'weight': weights_matrix})\n","    if non_trainable:\n","        emb_layer.weight.requires_grad = False\n","    return emb_layer, num_embeddings, embedding_dim"]},{"cell_type":"markdown","metadata":{},"source":["# models.py"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-10-31T15:14:32.549859Z","iopub.status.busy":"2023-10-31T15:14:32.549544Z","iopub.status.idle":"2023-10-31T15:14:32.840413Z","shell.execute_reply":"2023-10-31T15:14:32.839438Z","shell.execute_reply.started":"2023-10-31T15:14:32.549834Z"},"trusted":true},"outputs":[],"source":["import torch\n","from torch.nn import TransformerEncoder, TransformerEncoderLayer\n","\n","class ERC_MMN(nn.Module):\n","    def __init__(self,hidden_size,weight_matrix,utt2idx,batch_size,seq_len):\n","        super(ERC_MMN,self).__init__()\n","        self.ia = interact(hidden_size,weight_matrix,utt2idx)\n","        self.mn = memnet(4,hidden_size,batch_size,seq_len)\n","        self.pool = pool()\n","        \n","        self.rnn_c = myRNN(hidden_size*3,hidden_size*2,1)\n","        \n","        self.rnn_e = myRNN(hidden_size*2,hidden_size*2,1)\n","                \n","        self.linear1 = fc_e(hidden_size*2,7)\n","\n","    def forward(self,c_ids,speaker_info,sp_dialogues,sp_em,sp_ind,x1,mode=\"train\"):\n","        glob, splvl = self.ia(c_ids,speaker_info,sp_dialogues,sp_ind,x1)\n","\n","        op = self.mn(glob,splvl)\n","        op = self.pool(op)\n","\n","        op = torch.cat([splvl,op],dim=2)\n","\n","        rnn_c_op,_ = self.rnn_c(op)\n","\n","        rnn_e_op,_ = self.rnn_e(rnn_c_op)\n","        fip = rnn_e_op.add(rnn_c_op)      # Residual Connection\n","        fop1 = self.linear1(fip)\n","\n","        return fip,fop1\n","\n","class EFR_TX(nn.Module):\n","    def __init__(self, weight_matrix, utt2idx, nclass, ninp, count_speakers, nsp, nhead, nhid, nlayers, device, dropout=0.5):\n","        super(EFR_TX, self).__init__()\n","        self.model_type = 'Transformer'\n","        self.src_mask = None\n","        self.pos_encoder = PositionalEncoding(ninp, dropout)\n","        encoder_layers = TransformerEncoderLayer(ninp, nhead, nhid, dropout)\n","        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n","        self.encoder, num_embeddings, embedding_dim = create_emb_layer(weight_matrix, utt2idx)\n","        self.ninp = ninp\n","        self.decoder = nn.Linear(2*ninp, nclass)\n","        self.speakers_embedding = torch.nn.Embedding(count_speakers, nsp)\n","\n","        self.init_weights()\n","        self.device = device\n","\n","    def _generate_square_subsequent_mask(self, sz):\n","        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n","        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n","        return mask\n","\n","    def init_weights(self):\n","        initrange = 0.1\n","        self.encoder.weight.data.uniform_(-initrange, initrange)\n","        self.decoder.bias.data.zero_()\n","        self.decoder.weight.data.uniform_(-initrange, initrange)\n","\n","    def forward(self, src, d_ids, sp_ids, ut_len):\n","        device = 'cuda'\n","        torch.set_default_device('cuda')\n","        if self.src_mask is None or self.src_mask.size(0) != len(src):\n","            device = src.device\n","            mask = self._generate_square_subsequent_mask(len(src)).to(device)\n","            self.src_mask = mask\n","\n","        # Old Code\n","        # src = self.encoder(src) * math.sqrt(self.ninp)\n","        # New\n","        src = self.encoder(src)\n","        new_src = torch.zeros(src.shape[0],src.shape[1],self.ninp)\n","        for ix1,mat in enumerate(src):\n","            for ix2,vec in enumerate(mat):\n","                new_src[ix1][ix2] = torch.cat([self.speakers_embedding(torch.tensor(sp_ids[ix1][ix2], device=device, dtype=torch.long)), src[ix1][ix2]],-1)\n","        src = new_src\n","        src = src * math.sqrt(self.ninp)        \n","        \n","        src = self.pos_encoder(src)\n","        output = self.transformer_encoder(src, self.src_mask)\n","        \n","        decoder_ip = torch.zeros(output.size()[0],output.size()[1],output.size()[2]*2).to(self.device)\n","        for b in range(output.size()[0]):\n","            d_id = d_ids[b][0]\n","            main_utt = output[b][ut_len[d_id]-1]\n","            for s in range(ut_len[d_id]):\n","                this_utt = output[b][s]\n","                decoder_ip[b][s] = torch.cat([this_utt,main_utt],-1)\n","        \n","        output = self.decoder(decoder_ip)\n","        \n","        return decoder_ip,output\n","\n","class ERC_true_EFR(nn.Module):\n","    def __init__(self, weight_matrix, utt2idx, nclass, ninp, nhead, nhid, nlayers, dropout=0.5):\n","        super(ERC_true_EFR, self).__init__()\n","        self.model_type = 'Transformer'\n","        self.src_mask = None\n","        self.pos_encoder = PositionalEncoding(ninp, dropout)\n","        encoder_layers = TransformerEncoderLayer(ninp, nhead, nhid, dropout)\n","        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n","        self.encoder, num_embeddings, embedding_dim = create_emb_layer(weight_matrix,utt2idx)\n","        \n","        self.emoGRU = myRNN(7,100,1)\n","        self.ninp = ninp\n","        self.decoder = nn.Linear(2*ninp+100, nclass)\n","\n","        self.init_weights()\n","\n","    def _generate_square_subsequent_mask(self, sz):\n","        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n","        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n","        return mask\n","\n","    def init_weights(self):\n","        initrange = 0.1\n","        self.encoder.weight.data.uniform_(-initrange, initrange)\n","        self.decoder.bias.data.zero_()\n","        self.decoder.weight.data.uniform_(-initrange, initrange)\n","\n","    def forward(self, src, em_seq, d_ids, ut_len):\n","        if self.src_mask is None or self.src_mask.size(0) != len(src):\n","            device = src.device\n","            mask = self._generate_square_subsequent_mask(len(src)).to(device)\n","            self.src_mask = mask\n","\n","        src = self.encoder(src) * math.sqrt(self.ninp)\n","        src = self.pos_encoder(src)\n","        output = self.transformer_encoder(src, self.src_mask)\n","        \n","        emo_seq,_ = self.emoGRU(em_seq.float())\n","        \n","        decoder_ip = torch.zeros(output.size()[0],output.size()[1],output.size()[2]*2).cuda()\n","        for b in range(output.size()[0]):\n","            d_id = d_ids[b][0]\n","            main_utt = output[b][ut_len[d_id]-1]\n","            for s in range(ut_len[d_id]):\n","                this_utt = output[b][s]\n","                decoder_ip[b][s] = torch.cat([this_utt,main_utt],-1)\n","        \n","        decoder_ip = torch.cat([decoder_ip,emo_seq],-1)\n","        output = self.decoder(decoder_ip)\n","        \n","        return output\n","\n","class ERC_EFR_multitask(nn.Module):\n","    def __init__(self,hidden_size,weight_matrix,utt2idx,batch_size,seq_len):\n","        super(ERC_EFR_multitask,self).__init__()\n","        self.ia = interact(hidden_size,weight_matrix,utt2idx)\n","        self.mn = memnet(4,hidden_size,batch_size,seq_len)\n","        self.pool = pool()\n","        \n","        self.rnn_c = myRNN(hidden_size*3,hidden_size*2,1)\n","        \n","        self.rnn_e = myRNN(hidden_size*2,hidden_size*2,1)\n","        self.rnn_t = myRNN(hidden_size*2,hidden_size,1)\n","\n","        self.linear1 = fc_e(hidden_size*2,7)\n","        self.linear2 = fc_t(hidden_size*2,2)\n","\n","    def forward(self,c_ids,speaker_info,sp_dialogues,sp_em,sp_ind,freeze,x1,mode=\"train\"):\n","        speaker_emo = {}\n","        speaker_emo_distance = {}\n","        \n","        for d_id in c_ids:\n","            speaker_emo[d_id] = {}\n","            speaker_emo_distance[d_id] = {}\n","                    \n","        if freeze:\n","            with torch.no_grad():\n","                glob, splvl = self.ia(c_ids,speaker_info,sp_dialogues,sp_ind,x1)\n","        \n","                op = self.mn(glob,splvl)\n","                op = self.pool(op)\n","\n","                op = torch.cat([splvl,op],dim=2)\n","\n","                rnn_c_op,_ = self.rnn_c(op)\n","\n","                rnn_e_op,_ = self.rnn_e(rnn_c_op)\n","                rnn_e_op = rnn_e_op.add(rnn_c_op)      # Residual Connection\n","                fop1 = self.linear1(rnn_e_op)\n","        else:\n","            glob, splvl = self.ia(c_ids,speaker_info,sp_dialogues,sp_ind,x1)\n","        \n","            op = self.mn(glob,splvl)\n","            op = self.pool(op)\n","            \n","            op = torch.cat([splvl,op],dim=2)\n","\n","            rnn_c_op,_ = self.rnn_c(op)\n","\n","            rnn_e_op,_ = self.rnn_e(rnn_c_op)\n","            rnn_e_op = rnn_e_op.add(rnn_c_op)      # Residual Connection\n","            fop1 = self.linear1(rnn_e_op)\n","        \n","        rnn_t_op,_ = self.rnn_t(rnn_c_op)\n","\n","        fop2_final = []\n","        for b in range(rnn_t_op.size()[0]):\n","            d_id = c_ids[b]\n","            fop2_final_tmp = []\n","            for s in range(rnn_t_op.size()[1]):\n","                fop2_final_tmp_tmp = []\n","                concerned_utt = rnn_t_op[b][s]\n","                \n","                if s < 4:\n","                    r = s+1\n","                else:\n","                    r = 4\n","                \n","                for s2 in range(r,-1,-1):\n","                    this_utt = rnn_t_op[b][s-s2]\n","                    tmp = torch.cat((concerned_utt,this_utt),-1)\n","                    fop2 = self.linear2(tmp)\n","\n","                    fop2_final_tmp_tmp.append(fop2)\n","                fop2_final_tmp.append(fop2_final_tmp_tmp)\n","            fop2_final.append(fop2_final_tmp)\n","        return fop1,fop2_final\n","\n","class cascade(nn.Module):\n","    def __init__(self,hidden_size,nclasses):\n","        super(cascade,self).__init__()        \n","        self.linear = fc_e(hidden_size*4,nclasses)\n","    \n","    def forward(self,x1):\n","        op = self.linear(x1)\n","        return op"]},{"cell_type":"markdown","metadata":{},"source":["# Train EFR-TX.py"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-31T15:16:37.800855Z","iopub.status.busy":"2023-10-31T15:16:37.800410Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["4000\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e5d77a89e08a40efa2a4f2d8d5b55d73","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1000 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\n","\n","-------Epoch 1-------\n","\n","\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a636c01b67aa452b8a27b68fb6e33828","version_major":2,"version_minor":0},"text/plain":["0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Average Loss =  tensor([0.0205], device='cuda:0', grad_fn=<DivBackward0>)\n","\n","\n","***VALIDATION (0)***\n","\n","\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"26cab51c75e340fcb3ca5ad372d1216a","version_major":2,"version_minor":0},"text/plain":["0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.76      0.92      0.83      1497\n","           1       0.35      0.13      0.19       490\n","\n","    accuracy                           0.72      1987\n","   macro avg       0.55      0.53      0.51      1987\n","weighted avg       0.66      0.72      0.68      1987\n","\n","Confusion Matrix: \n"," [[1372  125]\n"," [ 424   66]]\n","\n","\n","-------Epoch 2-------\n","\n","\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"acdd6dccf99a4ecc8ed8358d9b7e33d3","version_major":2,"version_minor":0},"text/plain":["0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Average Loss =  tensor([0.0197], device='cuda:0', grad_fn=<DivBackward0>)\n","\n","\n","***VALIDATION (1)***\n","\n","\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b4d93ee302644a60957e0dc7a59f3a07","version_major":2,"version_minor":0},"text/plain":["0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.76      0.91      0.83      1497\n","           1       0.33      0.14      0.20       490\n","\n","    accuracy                           0.72      1987\n","   macro avg       0.55      0.53      0.51      1987\n","weighted avg       0.66      0.72      0.67      1987\n","\n","Confusion Matrix: \n"," [[1358  139]\n"," [ 420   70]]\n","\n","\n","-------Epoch 3-------\n","\n","\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"33a2fe80ef3c471ab1806f0d1b3c95cd","version_major":2,"version_minor":0},"text/plain":["0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Average Loss =  tensor([0.0181], device='cuda:0', grad_fn=<DivBackward0>)\n","\n","\n","***VALIDATION (2)***\n","\n","\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"27ef1aaf7ed14836a593d91f4b0c5102","version_major":2,"version_minor":0},"text/plain":["0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.76      0.90      0.82      1497\n","           1       0.32      0.15      0.21       490\n","\n","    accuracy                           0.71      1987\n","   macro avg       0.54      0.52      0.52      1987\n","weighted avg       0.65      0.71      0.67      1987\n","\n","Confusion Matrix: \n"," [[1342  155]\n"," [ 416   74]]\n","\n","\n","-------Epoch 4-------\n","\n","\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ae574a67cfbc43d08e1cf5fa35de5502","version_major":2,"version_minor":0},"text/plain":["0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Average Loss =  tensor([0.0176], device='cuda:0', grad_fn=<DivBackward0>)\n","\n","\n","***VALIDATION (3)***\n","\n","\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1e1bcf4b52404e4782fbfe8c825cee77","version_major":2,"version_minor":0},"text/plain":["0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.76      0.89      0.82      1497\n","           1       0.31      0.16      0.21       490\n","\n","    accuracy                           0.71      1987\n","   macro avg       0.54      0.52      0.52      1987\n","weighted avg       0.65      0.71      0.67      1987\n","\n","Confusion Matrix: \n"," [[1327  170]\n"," [ 412   78]]\n","\n","\n","-------Epoch 5-------\n","\n","\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"06721fef37d9423980adbf43127f433a","version_major":2,"version_minor":0},"text/plain":["0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Average Loss =  tensor([0.0168], device='cuda:0', grad_fn=<DivBackward0>)\n","\n","\n","***VALIDATION (4)***\n","\n","\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"13c95bc67f704bf9897fc8e30d0432b1","version_major":2,"version_minor":0},"text/plain":["0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.77      0.87      0.82      1497\n","           1       0.32      0.19      0.24       490\n","\n","    accuracy                           0.70      1987\n","   macro avg       0.54      0.53      0.53      1987\n","weighted avg       0.66      0.70      0.67      1987\n","\n","Confusion Matrix: \n"," [[1305  192]\n"," [ 399   91]]\n","\n","\n","-------Epoch 6-------\n","\n","\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"31a5934b2e864fee84abd3889ac9377a","version_major":2,"version_minor":0},"text/plain":["0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Average Loss =  tensor([0.0160], device='cuda:0', grad_fn=<DivBackward0>)\n","\n","\n","***VALIDATION (5)***\n","\n","\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4c8cd893d0f44d5c900859de48448ea7","version_major":2,"version_minor":0},"text/plain":["0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.77      0.86      0.81      1497\n","           1       0.31      0.20      0.24       490\n","\n","    accuracy                           0.69      1987\n","   macro avg       0.54      0.53      0.53      1987\n","weighted avg       0.65      0.69      0.67      1987\n","\n","Confusion Matrix: \n"," [[1280  217]\n"," [ 392   98]]\n","\n","\n","-------Epoch 7-------\n","\n","\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a52f523e23c346d0ba7ba90159f40549","version_major":2,"version_minor":0},"text/plain":["0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Average Loss =  tensor([0.0149], device='cuda:0', grad_fn=<DivBackward0>)\n","\n","\n","***VALIDATION (6)***\n","\n","\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"49e5b04720b4491da1790c24e632e833","version_major":2,"version_minor":0},"text/plain":["0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.77      0.83      0.80      1497\n","           1       0.30      0.22      0.26       490\n","\n","    accuracy                           0.68      1987\n","   macro avg       0.53      0.53      0.53      1987\n","weighted avg       0.65      0.68      0.66      1987\n","\n","Confusion Matrix: \n"," [[1246  251]\n"," [ 381  109]]\n","\n","\n","-------Epoch 8-------\n","\n","\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b4ae9ca8d82745338f78a711539e2bf5","version_major":2,"version_minor":0},"text/plain":["0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Average Loss =  tensor([0.0145], device='cuda:0', grad_fn=<DivBackward0>)\n","\n","\n","***VALIDATION (7)***\n","\n","\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5857da50642e40109bfb9a97e801a32b","version_major":2,"version_minor":0},"text/plain":["0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.77      0.82      0.79      1497\n","           1       0.30      0.24      0.27       490\n","\n","    accuracy                           0.68      1987\n","   macro avg       0.54      0.53      0.53      1987\n","weighted avg       0.65      0.68      0.66      1987\n","\n","Confusion Matrix: \n"," [[1228  269]\n"," [ 372  118]]\n","\n","\n","-------Epoch 9-------\n","\n","\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"bfd2f7638c54497db6acc7bfeb9823ba","version_major":2,"version_minor":0},"text/plain":["0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Average Loss =  tensor([0.0145], device='cuda:0', grad_fn=<DivBackward0>)\n","\n","\n","***VALIDATION (8)***\n","\n","\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0e4ed3e7a13448b1b3fbf0065c8fc217","version_major":2,"version_minor":0},"text/plain":["0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.77      0.80      0.78      1497\n","           1       0.30      0.26      0.27       490\n","\n","    accuracy                           0.67      1987\n","   macro avg       0.53      0.53      0.53      1987\n","weighted avg       0.65      0.67      0.66      1987\n","\n","Confusion Matrix: \n"," [[1199  298]\n"," [ 365  125]]\n","\n","\n","-------Epoch 10-------\n","\n","\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"eee1bd97374e448da41eca00db3e870a","version_major":2,"version_minor":0},"text/plain":["0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Average Loss =  tensor([0.0143], device='cuda:0', grad_fn=<DivBackward0>)\n","\n","\n","***VALIDATION (9)***\n","\n","\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"734068ecfc1e4bfb9decfcd5b4292f64","version_major":2,"version_minor":0},"text/plain":["0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.77      0.79      0.78      1497\n","           1       0.29      0.26      0.28       490\n","\n","    accuracy                           0.66      1987\n","   macro avg       0.53      0.53      0.53      1987\n","weighted avg       0.65      0.66      0.66      1987\n","\n","Confusion Matrix: \n"," [[1187  310]\n"," [ 361  129]]\n","\n","\n","-------Epoch 11-------\n","\n","\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"42726ecd681a4c93a0211b707bc99f0d","version_major":2,"version_minor":0},"text/plain":["0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Average Loss =  tensor([0.0127], device='cuda:0', grad_fn=<DivBackward0>)\n","\n","\n","***VALIDATION (10)***\n","\n","\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7a279a2600dd4652b6b2eb5d996d0053","version_major":2,"version_minor":0},"text/plain":["0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.77      0.78      0.77      1497\n","           1       0.29      0.28      0.28       490\n","\n","    accuracy                           0.65      1987\n","   macro avg       0.53      0.53      0.53      1987\n","weighted avg       0.65      0.65      0.65      1987\n","\n","Confusion Matrix: \n"," [[1162  335]\n"," [ 355  135]]\n","\n","\n","-------Epoch 12-------\n","\n","\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c7ff2543cef24a1f8567e4f073a6aadb","version_major":2,"version_minor":0},"text/plain":["0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Average Loss =  tensor([0.0159], device='cuda:0', grad_fn=<DivBackward0>)\n","\n","\n","***VALIDATION (11)***\n","\n","\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3171e43c2c434352814cfb518d1f2756","version_major":2,"version_minor":0},"text/plain":["0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.77      0.76      0.76      1497\n","           1       0.28      0.30      0.29       490\n","\n","    accuracy                           0.64      1987\n","   macro avg       0.53      0.53      0.53      1987\n","weighted avg       0.65      0.64      0.65      1987\n","\n","Confusion Matrix: \n"," [[1133  364]\n"," [ 345  145]]\n","\n","\n","-------Epoch 13-------\n","\n","\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c4f86bc7bb6a4f748dd57fe6dad73a1c","version_major":2,"version_minor":0},"text/plain":["0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Average Loss =  tensor([0.0135], device='cuda:0', grad_fn=<DivBackward0>)\n","\n","\n","***VALIDATION (12)***\n","\n","\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"22c3b552cb604e19813f342da0aa7cd5","version_major":2,"version_minor":0},"text/plain":["0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.76      0.73      0.75      1497\n","           1       0.28      0.31      0.29       490\n","\n","    accuracy                           0.63      1987\n","   macro avg       0.52      0.52      0.52      1987\n","weighted avg       0.64      0.63      0.64      1987\n","\n","Confusion Matrix: \n"," [[1096  401]\n"," [ 337  153]]\n","\n","\n","-------Epoch 14-------\n","\n","\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d1eeb1c3d4f44e04b4045e049a7df002","version_major":2,"version_minor":0},"text/plain":["0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Average Loss =  tensor([0.0134], device='cuda:0', grad_fn=<DivBackward0>)\n","\n","\n","***VALIDATION (13)***\n","\n","\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1a212b0d42474ef79643e3b6d7c7e6c6","version_major":2,"version_minor":0},"text/plain":["0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.77      0.72      0.74      1497\n","           1       0.28      0.33      0.30       490\n","\n","    accuracy                           0.62      1987\n","   macro avg       0.52      0.53      0.52      1987\n","weighted avg       0.65      0.62      0.63      1987\n","\n","Confusion Matrix: \n"," [[1076  421]\n"," [ 327  163]]\n","\n","\n","-------Epoch 15-------\n","\n","\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2344406e2fb6405badf2a22b92099079","version_major":2,"version_minor":0},"text/plain":["0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Average Loss =  tensor([0.0110], device='cuda:0', grad_fn=<DivBackward0>)\n","\n","\n","***VALIDATION (14)***\n","\n","\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d56f506a916e45baac05f56d6bc5a146","version_major":2,"version_minor":0},"text/plain":["0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.77      0.69      0.73      1497\n","           1       0.27      0.35      0.31       490\n","\n","    accuracy                           0.61      1987\n","   macro avg       0.52      0.52      0.52      1987\n","weighted avg       0.64      0.61      0.62      1987\n","\n","Confusion Matrix: \n"," [[1040  457]\n"," [ 319  171]]\n","\n","\n","-------Epoch 16-------\n","\n","\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e4e998c0ab5142209e8c6bf47e631295","version_major":2,"version_minor":0},"text/plain":["0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Average Loss =  tensor([0.0141], device='cuda:0', grad_fn=<DivBackward0>)\n","\n","\n","***VALIDATION (15)***\n","\n","\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"47b66d95f569412a956f829204908562","version_major":2,"version_minor":0},"text/plain":["0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.77      0.67      0.71      1497\n","           1       0.27      0.38      0.32       490\n","\n","    accuracy                           0.60      1987\n","   macro avg       0.52      0.52      0.52      1987\n","weighted avg       0.65      0.60      0.62      1987\n","\n","Confusion Matrix: \n"," [[1001  496]\n"," [ 304  186]]\n","\n","\n","-------Epoch 17-------\n","\n","\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"22b510321ab94f0cbdbe636ca25bfb69","version_major":2,"version_minor":0},"text/plain":["0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Average Loss =  tensor([0.0129], device='cuda:0', grad_fn=<DivBackward0>)\n","\n","\n","***VALIDATION (16)***\n","\n","\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"908fb0ad1b91458e8e6353a095acde39","version_major":2,"version_minor":0},"text/plain":["0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.77      0.64      0.70      1497\n","           1       0.28      0.42      0.33       490\n","\n","    accuracy                           0.58      1987\n","   macro avg       0.52      0.53      0.52      1987\n","weighted avg       0.65      0.58      0.61      1987\n","\n","Confusion Matrix: \n"," [[954 543]\n"," [283 207]]\n","\n","\n","-------Epoch 18-------\n","\n","\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"25feee8adbe84c04a5fae1664e9c60e4","version_major":2,"version_minor":0},"text/plain":["0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Average Loss =  tensor([0.0129], device='cuda:0', grad_fn=<DivBackward0>)\n","\n","\n","***VALIDATION (17)***\n","\n","\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"308f3e13d2e745f6b02bc61cdb88fd05","version_major":2,"version_minor":0},"text/plain":["0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.77      0.62      0.68      1497\n","           1       0.27      0.44      0.34       490\n","\n","    accuracy                           0.57      1987\n","   macro avg       0.52      0.53      0.51      1987\n","weighted avg       0.65      0.57      0.60      1987\n","\n","Confusion Matrix: \n"," [[921 576]\n"," [272 218]]\n","\n","\n","-------Epoch 19-------\n","\n","\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f788046ed2ad4cc5892506c009add7d0","version_major":2,"version_minor":0},"text/plain":["0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Average Loss =  tensor([0.0139], device='cuda:0', grad_fn=<DivBackward0>)\n","\n","\n","***VALIDATION (18)***\n","\n","\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"04dee81c2c8f4d108bf103dd3f5be0be","version_major":2,"version_minor":0},"text/plain":["0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.78      0.61      0.68      1497\n","           1       0.28      0.46      0.35       490\n","\n","    accuracy                           0.57      1987\n","   macro avg       0.53      0.53      0.51      1987\n","weighted avg       0.65      0.57      0.60      1987\n","\n","Confusion Matrix: \n"," [[911 586]\n"," [264 226]]\n","\n","\n","-------Epoch 20-------\n","\n","\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"44dc3c8f8090435cbec93fb92a8cbe7c","version_major":2,"version_minor":0},"text/plain":["0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Average Loss =  tensor([0.0110], device='cuda:0', grad_fn=<DivBackward0>)\n","\n","\n","***VALIDATION (19)***\n","\n","\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"12ef696110a842489cb5831f4e73a1f4","version_major":2,"version_minor":0},"text/plain":["0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.78      0.59      0.67      1497\n","           1       0.28      0.48      0.35       490\n","\n","    accuracy                           0.56      1987\n","   macro avg       0.53      0.53      0.51      1987\n","weighted avg       0.65      0.56      0.59      1987\n","\n","Confusion Matrix: \n"," [[879 618]\n"," [255 235]]\n","\n","\n","-------Epoch 21-------\n","\n","\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d2f7b12feaf049fcad12c6410a4cccdf","version_major":2,"version_minor":0},"text/plain":["0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Average Loss =  tensor([0.0110], device='cuda:0', grad_fn=<DivBackward0>)\n","\n","\n","***VALIDATION (20)***\n","\n","\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e4bc2d8c52774109945f7086516635b9","version_major":2,"version_minor":0},"text/plain":["0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.78      0.56      0.65      1497\n","           1       0.27      0.50      0.35       490\n","\n","    accuracy                           0.55      1987\n","   macro avg       0.52      0.53      0.50      1987\n","weighted avg       0.65      0.55      0.58      1987\n","\n","Confusion Matrix: \n"," [[841 656]\n"," [244 246]]\n","\n","\n","-------Epoch 22-------\n","\n","\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"928a54fff5de4f22b23441afded5d2f8","version_major":2,"version_minor":0},"text/plain":["0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Average Loss =  tensor([0.0114], device='cuda:0', grad_fn=<DivBackward0>)\n","\n","\n","***VALIDATION (21)***\n","\n","\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d7a80d7069484f5490005c6767b3c533","version_major":2,"version_minor":0},"text/plain":["0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.78      0.55      0.64      1497\n","           1       0.27      0.52      0.36       490\n","\n","    accuracy                           0.54      1987\n","   macro avg       0.53      0.53      0.50      1987\n","weighted avg       0.65      0.54      0.57      1987\n","\n","Confusion Matrix: \n"," [[819 678]\n"," [235 255]]\n","\n","\n","-------Epoch 23-------\n","\n","\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a759e3c4db3b476fa091d699d0b8e5fa","version_major":2,"version_minor":0},"text/plain":["0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Average Loss =  tensor([0.0137], device='cuda:0', grad_fn=<DivBackward0>)\n","\n","\n","***VALIDATION (22)***\n","\n","\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f7e58dc33ee84607a47989f78278d794","version_major":2,"version_minor":0},"text/plain":["0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.78      0.52      0.62      1497\n","           1       0.27      0.55      0.36       490\n","\n","    accuracy                           0.52      1987\n","   macro avg       0.52      0.53      0.49      1987\n","weighted avg       0.65      0.52      0.56      1987\n","\n","Confusion Matrix: \n"," [[773 724]\n"," [222 268]]\n","\n","\n","-------Epoch 24-------\n","\n","\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"728f2e98151e4862be62b99cc1a1cf6b","version_major":2,"version_minor":0},"text/plain":["0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from tqdm.auto import tqdm\n","from torch.utils import data\n","from sklearn.metrics import classification_report\n","from sklearn.metrics import f1_score\n","from sklearn.metrics import confusion_matrix\n","\n","batch_size = 128\n","seq_len = 5\n","seq2_len = seq_len\n","emb_size = 768\n","hidden_size = 768\n","batch_first = True\n","\n","torch.set_default_device('cuda')\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","idx2utt, utt2idx, idx2emo, emo2idx, idx2speaker,\\\n","        speaker2idx, weight_matrix, my_dataset_train, my_dataset_test,\\\n","        global_speaker_info, speaker_dialogues, speaker_emotions, \\\n","        speaker_indices, utt_len, global_speaker_info_test, speaker_dialogues_test, \\\n","        speaker_emotions_test, speaker_indices_test, utt_len_test = load_efr()\n","    \n","def get_train_test_loader(bs):\n","    print(len(my_dataset_train))\n","    train_data_iter = data.DataLoader(my_dataset_train, batch_size=bs)\n","    test_data_iter = data.DataLoader(my_dataset_test, batch_size=bs)\n","    \n","    return train_data_iter, test_data_iter\n","    \n","def train(model, train_data_loader, epochs):\n","    class_weights2 = torch.FloatTensor(weights2).to(device)\n","    criterion2 = nn.CrossEntropyLoss(weight=class_weights2,reduction='none').to(device)\n","    \n","    optimizer = torch.optim.Adam(model.parameters(),lr=5e-8,weight_decay=1e-5)\n","    \n","    max_f1_2 = 0\n","   \n","    for epoch in tqdm(range(epochs)):\n","        print(\"\\n\\n-------Epoch {}-------\\n\\n\".format(epoch+1))\n","        model.train()\n","        \n","        avg_loss = 0\n","       \n","        y_true2 = []\n","        y_pred2 = []\n","            \n","        for i_batch, sample_batched in tqdm(enumerate(train_data_loader)):\n","            dialogue_ids = sample_batched[0].tolist()\n","            inputs = sample_batched[1].to(device)\n","            targets2 = sample_batched[3].to(device)\n","            \n","            # Creating the speaker_ids\n","            speaker_ids = []\n","            for d_ids_list in dialogue_ids:\n","              sp_id_list = [0] * len(d_ids_list)\n","              for ix, d_id in enumerate(d_ids_list):\n","                sp_id = global_speaker_info[d_id][0]\n","                sp_id_list[ix] = sp_id\n","              speaker_ids.append(sp_id_list)\n","            \n","            optimizer.zero_grad()\n","            \n","            _,outputs = model(inputs,dialogue_ids,speaker_ids,utt_len)\n","            \n","            loss = 0\n","            for b in range(outputs.size()[0]):\n","              loss2 = 0\n","              \n","              for s in range(utt_len[dialogue_ids[b][0]]):\n","                pred2 = outputs[b][s]\n","                pred_flip = torch.argmax(F.softmax(pred2.to(device),-1),-1)\n","                \n","                truth2 = targets2[b][s]\n","\n","                y_pred2.append(pred_flip.item())\n","                y_true2.append(truth2.long().to(device).item())\n","\n","                pred2_ = torch.unsqueeze(pred2,0)\n","                truth2_ = torch.unsqueeze(truth2,0)\n","                \n","                loss2 += criterion2(pred2_,truth2_)\n","              loss2 /= utt_len[dialogue_ids[b][0]]\n","            \n","            loss += loss2\n","            loss /= outputs.size()[0]\n","            avg_loss += loss\n","\n","            loss.backward()            \n","            optimizer.step()\n","            \n","        avg_loss /= len(train_data_loader)\n","        \n","        print(\"Average Loss = \",avg_loss)\n","\n","        f1_2_cls,v_loss = validate(model, data_iter_test, epoch)\n","        \n","        # if f1_2_cls[1] > max_f1_2:\n","        #     print(f\"Saving model at epoch {epoch}\")\n","        #     max_f1_2 = f1_2_cls[1]\n","        #     torch.save(model.state_dict(), \"./best_model.pth\")\n","\n","    return model\n","\n","def validate(model, test_data_loader,epoch):\n","    print(\"\\n\\n***VALIDATION ({})***\\n\\n\".format(epoch))\n","    \n","    class_weights2 = torch.FloatTensor(weights2).to(device)\n","    criterion2 = nn.CrossEntropyLoss(weight=class_weights2,reduction='none').to(device)\n","\n","    model.eval()\n","\n","    with torch.no_grad():\n","      avg_loss = 0\n","      y_true2 = []\n","      y_pred2 = []\n","\n","      for i_batch, sample_batched in tqdm(enumerate(test_data_loader)):\n","            dialogue_ids = sample_batched[0].tolist()           \n","            inputs = sample_batched[1].to(device)\n","            targets2 = sample_batched[3].to(device)\n","            \n","            # Creating the speaker_ids\n","            speaker_ids = []\n","            for d_ids_list in dialogue_ids:\n","              sp_id_list = [0] * len(d_ids_list)\n","              for ix, d_id in enumerate(d_ids_list):\n","                sp_id = global_speaker_info[d_id][0]\n","                sp_id_list[ix] = sp_id\n","              speaker_ids.append(sp_id_list)\n","                       \n","            _,outputs = model(inputs,dialogue_ids,speaker_ids,utt_len)\n","            \n","            loss = 0\n","            for b in range(outputs.size()[0]):\n","              loss2 = 0\n","              \n","              for s in range(utt_len_test[dialogue_ids[b][0]]):\n","                pred2 = outputs[b][s]\n","                pred_flip = torch.argmax(F.softmax(pred2.to(device),-1),-1)\n","                \n","                truth2 = targets2[b][s]\n","\n","                y_pred2.append(pred_flip.item())\n","                y_true2.append(truth2.long().to(device).item())\n","\n","                pred2_ = torch.unsqueeze(pred2,0)\n","                truth2_ = torch.unsqueeze(truth2,0)\n","                \n","                loss2 += criterion2(pred2_,truth2_)\n","              loss2 /= utt_len_test[dialogue_ids[b][0]]\n","            \n","            loss += loss2\n","            loss /= outputs.size()[0]\n","            avg_loss += loss\n","\n","      avg_loss /= len(test_data_loader)\n","\n","      class_report = classification_report(y_true2,y_pred2)\n","      conf_mat2 = confusion_matrix(y_true2,y_pred2)\n","\n","      print(class_report)\n","      print(\"Confusion Matrix: \\n\",conf_mat2)\n","    \n","      f1 = f1_score(y_true2,y_pred2)\n","      return f1,avg_loss\n","\n","nclass = 2\n","utt_emsize = 768\n","personality_size = 100\n","nhid = 768\n","nlayers = 6\n","nhead = 2\n","dropout = 0.2\n","count_speakers = len(speaker2idx)\n","model = EFR_TX(weight_matrix, utt2idx, nclass, personality_size + utt_emsize, count_speakers, personality_size, nhead, nhid, nlayers, device, dropout).to(device)\n","\n","weights2 = [1.0, 2.5]\n","data_iter_train, data_iter_test = get_train_test_loader(batch_size)\n","model = train(model, data_iter_train, epochs = 1000)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
